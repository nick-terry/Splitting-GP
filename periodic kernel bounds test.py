# -*- coding: utf-8 -*-
"""
Created on Fri Jan  3 14:51:26 2020

@author: pnter
"""

import math
import torch
import gpytorch
from matplotlib import pyplot as plt
from varBoundFunctions import *
import numpy as np

def x_to_y(x):
    return torch.sin(2*math.pi*x + math.pi/4) + torch.sin(2*math.pi*x) + 5

x_start = 0
x_end = 10
x_num = 150

#Compute the difference between x values generated by linspace function
tau = (x_end-x_start)/(x_num-2)
train_x = torch.linspace(x_start,x_end,x_num)



#Randomly sample from these points
'''
rand_columns = torch.randperm(train_x.shape[0])[:100].sort().values
train_x = train_x[rand_columns]
'''
train_y = x_to_y(train_x)

#Note this kernel is isotropic but not decreasing, so the weaker Lipschitz bound must be used
kernel = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4)
#kernel = gpytorch.kernels.RBFKernel()

class PeriodicGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(PeriodicGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = kernel
        self.covar_module.initialize_from_data(train_x,train_y)
        
        #These properties are for computing the posterior variance bounds
        self.k = None
        self.kinv = None
        self.kinvInnerSums = None

    def forward(self,x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
    
    #Compute the posterior variance bounds for an **isotropic** kernel
    #
    #"direct" method computes posterior variance bounds directly using the 
    #inverse kernel matrix and the assumption that distance between consecutive
    #training data is at most tau apart
    #
    #"eigen" method computes the bounds using the maximal eigenvalue of the kernel matrix
    #
    #TODO:
    #"gersh" uses the Gershgorin theorem to get a looser bound which does not require finding the eigenvalues of the kernel
    def postVarBound(self,x,train_x,**kwargs):
        #If the kernel matrix is not yet computed, compute it
        n = train_x.shape[0]
        if self.k is None:
            #Result should be square nxn tensor
            train_x = train_x.view([n,1])
            self.k = self.covar_module(train_x,train_x).evaluate()
        
        if kwargs['method'] is None:
            kwargs['method'] = 'direct'
        
        #Compute raw bounds for posterior variance directly by inverting the kernel matrix
        if kwargs['method'] is 'direct':
            if self.kinv is None:
                self.kinv = torch.inverse(self.k)
                
            #Compute the inner summations needed for the posterior variance bounds.
            #Result will be a jx1 tensor where result[j] is the sum of elements in 
            #jth column of kinv
            def innerSums():
                return torch.sum(self.kinv,dim=0)
             
            if self.kinvInnerSums is None:
                self.kinvInnerSums = innerSums()
            
            #Define (n-1)x1 tensor of values at which to evaluate the kernel
            
            boundVals = torch.tensor([((n-j+1)*tau)**2 for j in range(1,n)]).view([n-1,1])
            
            #Compute kernel evaluated for |x-x_n|, then add to kernel evaluated at boundVals and multiply by corresponding column sum from kinv
            #k_x_xn = self.covar_module(x.view([1,1]),train_x[-1].view([1,1]))**2
            k_x_xn = self.covar_module(x.view([1,1]),train_x[-1].view([1,1])).evaluate()**2
            outerSumTerms = self.covar_module(boundVals,torch.zeros(n-1).view([n-1,1]),diag=True)**2*self.kinvInnerSums[:-1]
            #Compute kernel evaluated at d=0, then return bounds
            zero_tensor = torch.zeros([1,1])
            k_0 = self.covar_module(zero_tensor,zero_tensor).evaluate()
            return k_0 - torch.sum(outerSumTerms) - k_x_xn * self.kinvInnerSums[-1]
        
        if kwargs['method'] is 'eigen':
            #Compute kernel evaluated at d=0
            zero_tensor = torch.zeros([1,1])
            k_0 = self.covar_module(zero_tensor,zero_tensor).evaluate()
            
            #Find eigenvalues of k. We can safely take the real part since
            #k is positive definite and therefore has only positive real eigenvalues.
            eigVals = np.real(np.linalg.eigvals(self.k.detach()))
            
            #Return bound
            return float(k_0 - (torch.norm(self.covar_module(x+torch.zeros(n),train_x,diag=True))**2)/np.max(eigVals))
    
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = PeriodicGPModel(train_x, train_y, likelihood)

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

training_iter = 200
for i in range(training_iter):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()
    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))
    optimizer.step()
    


test_x = torch.linspace(x_start, x_end*5, int(5*(x_end-x_start)/tau+2))
test_y = x_to_y(test_x)

# Get into evaluation (predictive posterior) mode
model.eval()
likelihood.eval()

#Get the posterior variance bound function. Depends on the kernel hyperparameter, so it must be retrieved after training
#Note that we take rho=2*pi since the target function is sinusoidal, 
#so anything outside that radius will be aliased.
boundFn = getPostVarBoundFn(train_x.numpy(),0,2*math.pi,kernel)
boundFnTest = getPostVarBoundFnTest(train_x.numpy(),0,2*math.pi,kernel)
# The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)
# See https://arxiv.org/abs/1803.06058

y = list(map(boundFn,test_x.numpy()))
y = list(map(lambda x:x.detach().numpy(),y))
bound_y = np.array(y)
#This is a temporary hack to deal with NAN values. Replace NAN with earlier values since the bound is just constant
bound_y = [bound_y[0] if math.isnan(x) else x for x in bound_y]

yTest = list(map(boundFnTest,test_x.numpy()))
yTest = list(map(lambda x:x.detach().numpy(),yTest))
bound_yTest = np.array(yTest)

eigenBound = list(map(lambda x: float(model.postVarBound(x,train_x,method='eigen')),test_x.numpy()))
eigenBound = np.array(eigenBound)

'''
directBound = list(map(lambda x: float(model.postVarBound(x,train_x,method='direct')),test_x))
directBound = np.array(directBound)
'''

with torch.no_grad(), gpytorch.settings.fast_pred_var():
    # Make predictions
    observed_pred = likelihood(model(test_x))

    # Initialize plot
    f, ax = plt.subplots(1, 1, figsize=(4, 3))

    # Get upper and lower confidence bounds
    #lower, upper = observed_pred.confidence_region()
    var = model(test_x).variance.detach().numpy()
    #If there are any (slightly) negative entries due to imprecision, round to 0
    var = np.where(var<0,0,var)
    lower,upper = observed_pred.mean.numpy()-2*np.sqrt(var),observed_pred.mean.numpy()+2*np.sqrt(var)
    
    # Plot training data as black stars
    ax.plot(train_x.numpy(), train_y.numpy(), 'k*', label='Training Data')
    # Plot predictive means as red line
    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'r', label='Prediction (Mean of Posterior Distribution)')
    # Shade between the lower and upper confidence bounds
    ax.fill_between(test_x.numpy(), lower, upper, alpha=0.5, label='2*Sigma Confidence Region')
    #Plot ground truth function
    #ax.plot(test_x.numpy(),x_to_y(test_x).numpy(), 'y*')
    #Plot the bounds on posterior variance
    
    ax.plot(test_x.numpy(), observed_pred.mean.numpy()+2*np.sqrt(bound_y), 'g-', label='Posterior Bound - Lederer et al.')
    line = ax.plot(test_x.numpy(), observed_pred.mean.numpy()-2*np.sqrt(bound_y), 'g-')
    
    ax.plot(test_x.numpy(), observed_pred.mean.numpy()+2*np.sqrt(eigenBound), 'y-', label='Posterior Bound - Eigenvalue Method')
    line = ax.plot(test_x.numpy(), observed_pred.mean.numpy()-2*np.sqrt(eigenBound), 'y-')
    
    '''
    ax.plot(test_x.numpy(), observed_pred.mean.numpy()+2*np.sqrt(directBound), 'b-',  label='Posterior Bound - Direct Method')
    line = ax.plot(test_x.numpy(), observed_pred.mean.numpy()-2*np.sqrt(directBound), 'b-')
    '''
    
    ax.set_xlim([0, 20])
    ax.set_ylim([-5, 12])
    ax.legend()

    f2, ax2 = plt.subplots(1, 1, figsize=(4, 3))
    ax2.plot(test_x.numpy(),var, 'k')
    ax2.plot(test_x.numpy(),bound_y, 'g')
    ax2.plot(test_x.numpy(),eigenBound, 'y')
    #ax2.plot(test_x.numpy(),bound_yTest, 'y')
    
    
    f3, ax3 = plt.subplots(1, 1, figsize=(4, 3))
    ax3.plot(test_x.numpy(),observed_pred.mean.numpy()+2*np.sqrt(var), label='Direct computation of confidence region')
    #ax3.plot(test_x.numpy(),upper, label='Helper method')
    ax3.plot(test_x.numpy(), observed_pred.mean.numpy()+2*np.sqrt(eigenBound), label='Eigen Bound')
    ax3.plot(test_x.numpy(), observed_pred.mean.numpy()+2*np.sqrt(bound_y), label='Lederer et al. Bound')
    ax3.legend()
