
@article{huber_recursive_2014,
	title = {Recursive {Gaussian} process: {On}-line regression and learning},
	volume = {45},
	issn = {0167-8655},
	shorttitle = {Recursive {Gaussian} process},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865514000786},
	doi = {10.1016/j.patrec.2014.03.004},
	abstract = {Two approaches for on-line Gaussian process regression with low computational and memory demands are proposed. The first approach assumes known hyperparameters and performs regression on a set of basis vectors that stores mean and covariance estimates of the latent function. The second approach additionally learns the hyperparameters on-line. For this purpose, techniques from nonlinear Gaussian state estimation are exploited. The proposed approaches are compared to state-of-the-art sparse Gaussian process algorithms.},
	language = {en},
	urldate = {2019-12-13},
	journal = {Pattern Recognition Letters},
	author = {Huber, Marco F.},
	month = aug,
	year = {2014},
	keywords = {Gaussian processes, Kalman filtering, Parameter learning},
	pages = {85--91},
	file = {ScienceDirect Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\PK4FMZEU\\S0167865514000786.html:text/html}
}

@misc{murray-smith_adaptive_2003,
	type = {Conference {Proceedings}},
	title = {Adaptive, cautious, predictive control with {Gaussian} process priors},
	url = {http://eprints.gla.ac.uk/3112/},
	abstract = {Nonparametric Gaussian Process models, a Bayesian statistics approach, are used to implement a nonlinear adaptive control law. Predictions, including propagation of the state uncertainty are made over a k-step horizon. The expected value of a quadratic cost function is minimised, over this prediction horizon, without ignoring the variance of the model predictions. The general method and its main features are illustrated on a simulation example.},
	language = {en},
	urldate = {2019-12-13},
	author = {Murray-Smith, R. and Sbarbaro, D. and Rasmussen, C. E. and Girard, A.},
	year = {2003},
	file = {Full Text PDF:C\:\\Users\\pnter\\Zotero\\storage\\47NFCTQG\\Murray-Smith et al. - 2003 - Adaptive, cautious, predictive control with Gaussi.pdf:application/pdf;Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\BFG9GG2K\\3112.html:text/html}
}

@article{chen_dealing_2014,
	title = {Dealing with {Observation} {Outages} within {Navigation} {Data} using {Gaussian} {Process} {Regression}},
	volume = {67},
	issn = {0373-4633, 1469-7785},
	url = {https://www.cambridge.org/core/journals/journal-of-navigation/article/dealing-with-observation-outages-within-navigation-data-using-gaussian-process-regression/0D367C554E3DEECCAA6F608F5449CCED},
	doi = {10.1017/S0373463314000010},
	abstract = {Gaussian process regression (GPR) is used in a Spare-grid Quadrature Kalman filter (SGQKF) for Strap-down Inertial Navigation System (SINS)/odometer integrated navigation to bridge uncertain observation outages and maintain an estimate of the evolving SINS biases. The SGQKF uses nonlinearized dynamic models with complex stochastic nonlinearities so the performance degrades significantly during observation outages owing to the uncertainties and noise. The GPR calculates the residual output after factoring in the contributions of the parametric model that is used as a nonlinear SINS error predictor integrated into the SGQKF. The sensor measurements and SINS output deviations from the odometer are collected in a data set during observation availability. The GPR is then applied to predict SINS deviations from the odometer and then the predicted SINS deviations are fed to the SGQKF as an actual update to estimate all SINS biases during observation outages. We demonstrate our method's effectiveness in bridging uncertain observation outages in simulations and in real road tests. The results agree with the theoretical analysis, which demonstrate that SGQKF using GPR can maintain an estimate of the evolving SINS biases during signal outages.},
	language = {en},
	number = {4},
	urldate = {2019-12-13},
	journal = {The Journal of Navigation},
	author = {Chen, Hongmei and Cheng, Xianghong and Wang, Haipeng and Han, Xu},
	month = jul,
	year = {2014},
	keywords = {Gaussian process regression, Integrated navigation systems, Sparse-grid quadrature Kalman filter (SGQKF), Uncertain observations},
	pages = {603--615},
	file = {Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\I52HUABP\\0D367C554E3DEECCAA6F608F5449CCED.html:text/html}
}

@article{ye_inverse_2014,
	title = {The {Inverse} {Gaussian} {Process} as a {Degradation} {Model}},
	volume = {56},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2013.830074},
	doi = {10.1080/00401706.2013.830074},
	abstract = {This article systematically investigates the inverse Gaussian (IG) process as an effective degradation model. The IG process is shown to be a limiting compound Poisson process, which gives it a meaningful physical interpretation for modeling degradation of products deteriorating in random environments. Treated as the first passage process of a Wiener process, the IG process is flexible in incorporating random effects and explanatory variables that account for heterogeneities commonly observed in degradation problems. This flexibility makes the class of IG process models much more attractive compared with the Gamma process, which has been thoroughly investigated in the literature of degradation modeling. The article also discusses statistical inference for three random effects models and model selection. It concludes with a real world example to demonstrate the applicability of the IG process in degradation analysis. Supplementary materials for this article are available online.},
	number = {3},
	urldate = {2019-12-13},
	journal = {Technometrics},
	author = {Ye, Zhi-Sheng and Chen, Nan},
	month = jul,
	year = {2014},
	keywords = {Compound Poisson approximation, Explanatory variables, Monotone degradation paths, Random effects, Wiener process},
	pages = {302--311},
	file = {Full Text PDF:C\:\\Users\\pnter\\Zotero\\storage\\NLJBKG5D\\Ye and Chen - 2014 - The Inverse Gaussian Process as a Degradation Mode.pdf:application/pdf;Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\9HJJ2NMD\\00401706.2013.html:text/html}
}

@incollection{li_gaussian_2001,
	series = {Stochastic {Processes}: {Theory} and {Methods}},
	title = {Gaussian processes: {Inequalities}, small ball probabilities and applications},
	volume = {19},
	shorttitle = {Gaussian processes},
	url = {http://www.sciencedirect.com/science/article/pii/S016971610119019X},
	abstract = {This chapter focuses on the inequalities, small ball probabilities, and application of Gaussian processes. It is well-known that the large deviation result plays a fundamental role in studying the upper limits of Gaussian processes, such as the Strassen type law of the iterated logarithm. However, the complexity of the small ball estimate is well-known, and there are only a few Gaussian measures for which the small ball probability can be determined completely. The small ball probability is a key step in studying the lower limits of the Gaussian process. It has been found that the small ball estimate has close connections with various approximation quantities of compact sets and operators, and has a variety of applications in studies of Hausdorff dimensions, rate of convergence in Strassen's law of the iterated logarithm, and empirical processes.},
	language = {en},
	urldate = {2019-12-13},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Li, W. V. and Shao, Q. -M.},
	month = jan,
	year = {2001},
	doi = {10.1016/S0169-7161(01)19019-X},
	pages = {533--597},
	file = {ScienceDirect Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\2R4M8UHX\\S016971610119019X.html:text/html}
}

@article{deisenroth_robust_2012,
	title = {Robust {Filtering} and {Smoothing} with {Gaussian} {Processes}},
	volume = {57},
	issn = {2334-3303},
	doi = {10.1109/TAC.2011.2179426},
	abstract = {We propose a principled algorithm for robust Bayesian filtering and smoothing in nonlinear stochastic dynamic systems when both the transition function and the measurement function are described by non-parametric Gaussian process (GP) models. GPs are gaining increasing importance in signal processing, machine learning, robotics, and control for representing unknown system functions by posterior probability distributions. This modern way of system identification is more robust than finding point estimates of a parametric function representation. Our principled filtering/smoothing approach for GP dynamic systems is based on analytic moment matching in the context of the forward-backward algorithm. Our numerical evaluations demonstrate the robustness of the proposed approach in situations where other state-of-the-art Gaussian filters and smoothers can fail.},
	number = {7},
	journal = {IEEE Transactions on Automatic Control},
	author = {Deisenroth, Marc Peter and Turner, Ryan Darby and Huber, Marco F. and Hanebeck, Uwe D. and Rasmussen, Carl Edward},
	month = jul,
	year = {2012},
	keywords = {smoothing, Bayes methods, Gaussian processes, Training, Time measurement, Noise, machine learning, nonlinear systems, Robustness, analytic moment matching, Approximation methods, Bayesian inference, control systems, Covariance matrix, filtering, forward-backward algorithm, GP dynamic systems, identification, measurement function, nonlinear dynamical systems, nonlinear stochastic dynamic systems, nonparametric Gaussian process, nonparametric statistics, parametric function representation, point estimation, posterior probability distributions, robotics, robust Bayesian filtering, robust Bayesian smoothing, signal processing, smoothing methods, Smoothing methods, statistical distributions, system identification, transition function, unknown system function representation},
	pages = {1865--1871},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\pnter\\Zotero\\storage\\LFV6VQPS\\6099561.html:text/html;Submitted Version:C\:\\Users\\pnter\\Zotero\\storage\\SIWQEBNX\\Deisenroth et al. - 2012 - Robust Filtering and Smoothing with Gaussian Proce.pdf:application/pdf}
}

@article{sinopoli_kalman_2004,
	title = {Kalman filtering with intermittent observations},
	volume = {49},
	issn = {2334-3303},
	doi = {10.1109/TAC.2004.834121},
	abstract = {Motivated by navigation and tracking applications within sensor networks, we consider the problem of performing Kalman filtering with intermittent observations. When data travel along unreliable communication channels in a large, wireless, multihop sensor network, the effect of communication delays and loss of information in the control loop cannot be neglected. We address this problem starting from the discrete Kalman filtering formulation, and modeling the arrival of the observation as a random process. We study the statistical convergence properties of the estimation error covariance, showing the existence of a critical value for the arrival rate of the observations, beyond which a transition to an unbounded state error covariance occurs. We also give upper and lower bounds on this expected state error covariance.},
	number = {9},
	journal = {IEEE Transactions on Automatic Control},
	author = {Sinopoli, B. and Schenato, L. and Franceschetti, M. and Poolla, K. and Jordan, M.I. and Sastry, S.S.},
	month = sep,
	year = {2004},
	keywords = {Kalman filters, state estimation, adaptive Kalman filters, covariance analysis, Filtering, wireless sensor networks, Communication channels, Communication system control, convergence, Convergence, covariance matrices, Delay effects, discrete Kalman filtering, estimation error covariance, intermittent observations, multihop sensor networks, Navigation, online adaptive filtering, random processes, Random processes, Spread spectrum communication, statistical convergence, telecommunication channels, telecommunication control, unbounded state error covariance, unreliable communication channels, Wireless sensor networks},
	pages = {1453--1464},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\pnter\\Zotero\\storage\\W4RLTAYR\\1333199.html:text/html}
}

@article{evans_hidden_1999,
	title = {Hidden {Markov} model state estimation with randomly delayed observations},
	volume = {47},
	issn = {1941-0476},
	doi = {10.1109/78.774757},
	abstract = {This paper considers state estimation for a discrete-time hidden Markov model (HMM) when the observations are delayed by a random time. The delay process is itself modeled as a finite state Markov chain that allows an augmented state HMM to model the overall system. State estimation algorithms for the resulting HMM are then presented, and their performance is studied in simulations. The motivation for the model stems from the situation when distributed sensors transmit measurements over a connectionless packet switched communications network.},
	number = {8},
	journal = {IEEE Transactions on Signal Processing},
	author = {Evans, J.S. and Krishnamurthy, V.},
	month = aug,
	year = {1999},
	keywords = {state estimation, State estimation, filtering theory, Filters, filtering, Delay effects, random processes, augmented state HMM, Australia, Biomedical measurements, Communication networks, connectionless packet switched communications network, Delay estimation, delays, discrete-time hidden Markov model, distributed sensors, finite state Markov chain, hidden Markov model state estimation, hidden Markov models, Hidden Markov models, measurements transmission, packet switching, performance, randomly delayed observations, recursive filter, recursive filters, Sensor systems, Signal processing, simulations, state estimation algorithms, telecommunication networks},
	pages = {2157--2166},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\pnter\\Zotero\\storage\\EPU5NCDW\\774757.html:text/html}
}

@article{schwaighofer_transductive_nodate,
	title = {Transductive and {Inductive} {Methods} for {Approximate} {Gaussian} {Process} {Regression}},
	abstract = {Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we compare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian committee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves signiﬁcantly better accuracy, yet at a higher computational cost.},
	language = {en},
	author = {Schwaighofer, Anton and Tresp, Volker},
	pages = {8},
	file = {Schwaighofer and Tresp - Transductive and Inductive Methods for Approximate.pdf:C\:\\Users\\pnter\\Zotero\\storage\\3MJ4TYML\\Schwaighofer and Tresp - Transductive and Inductive Methods for Approximate.pdf:application/pdf}
}

@article{robinson_variational_1992,
	title = {Variational bounds on the entries of the inverse of a matrix},
	volume = {12},
	issn = {0272-4979, 1464-3642},
	url = {https://academic.oup.com/imajna/article-lookup/doi/10.1093/imanum/12.4.463},
	doi = {10.1093/imanum/12.4.463},
	language = {en},
	number = {4},
	urldate = {2019-12-15},
	journal = {IMA Journal of Numerical Analysis},
	author = {Robinson, P. D. and Wathen, A. J.},
	year = {1992},
	pages = {463--486},
	annote = {Some results from this paper may be useful to bound the inverse kernel matrix K{\textasciicircum}-1},
	file = {Robinson and Wathen - 1992 - Variational bounds on the entries of the inverse o.pdf:C\:\\Users\\pnter\\Zotero\\storage\\SZLHZZPZ\\Robinson and Wathen - 1992 - Variational bounds on the entries of the inverse o.pdf:application/pdf}
}

@article{zhang_sequential_2019,
	title = {Sequential {Gaussian} {Processes} for {Online} {Learning} of {Nonstationary} {Functions}},
	url = {http://arxiv.org/abs/1905.10003},
	abstract = {Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their ﬂexibility and uncertainty quantiﬁcation. However, the typical GP regression model suﬀers from several drawbacks: i) Conventional GP inference scales O(N 3) with respect to the number of observations; ii) updating a GP model sequentially is not trivial; and iii) covariance kernels often enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose an online sequential Monte Carlo algorithm to ﬁt mixtures of GPs that capture non-stationary behavior while allowing for fast, distributed inference. By formulating hyperparameter optimization as a multi-armed bandit problem, we accelerate mixing for real time inference. Our approach empirically improves performance over state-ofthe-art methods for online GP estimation in the context of prediction for simulated non-stationary data and hospital time series data.},
	language = {en},
	urldate = {2019-12-18},
	journal = {arXiv:1905.10003 [cs, stat]},
	author = {Zhang, Michael Minyi and Dumitrascu, Bianca and Williamson, Sinead A. and Engelhardt, Barbara E.},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.10003},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhang et al. - 2019 - Sequential Gaussian Processes for Online Learning .pdf:C\:\\Users\\pnter\\Zotero\\storage\\Y4W7U838\\Zhang et al. - 2019 - Sequential Gaussian Processes for Online Learning .pdf:application/pdf}
}

@inproceedings{koppel_consistent_2019,
	address = {Philadelphia, PA, USA},
	title = {Consistent {Online} {Gaussian} {Process} {Regression} {Without} the {Sample} {Complexity} {Bottleneck}},
	isbn = {978-1-5386-7926-5},
	url = {https://ieeexplore.ieee.org/document/8815206/},
	doi = {10.23919/ACC.2019.8815206},
	abstract = {Gaussian process regression provides a framework for nonlinear nonparametric Bayesian inference applicable across machine learning, robotics, chemical engineering, and other settings. Unfortunately, the computational burden of the posterior mean and covariance scales cubically with the training sample size. Even worse, in the online setting where samples perpetually arrive, this complexity approaches inﬁnity. Thus, popular perception is that Gaussian processes cannot be used with streaming data, and that approximations are required. Motivated by this necessity, we develop the ﬁrst compression sub-routine for online Gaussian processes that preserves their convergence to the population posterior, i.e., asymptotic posterior consistency, while ameliorating their intractable complexity growth with the sample size. We do so by after each sequential Bayesian update, ﬁxing an error neighborhood with respect to the Hellinger metric centered at the current empirical probability measure, and greedily tossing out past kernel dictionary elements until we hit the boundary of this neighborhood. We call the resulting method Parsimonious Online Gaussian Processes (POG). When we set the error radius, or compression budget, go to null with the sample size, then exact asymptotic consistency is preserved (Theorem 1i) at the cost of unbounded memory in the limit. On the other hand, for constant compression budget, POG converges to a neighborhood of the population posterior distribution (Theorem 1ii) but with ﬁnite memory that is at-worst determined by the metric entropy of the feature space (Theorem 2). Experiments on benchmark data demonstrates that POG exhibits favorable performance in practice.},
	language = {en},
	urldate = {2019-12-18},
	booktitle = {2019 {American} {Control} {Conference} ({ACC})},
	publisher = {IEEE},
	author = {Koppel, Alec},
	month = jul,
	year = {2019},
	pages = {3512--3518},
	file = {Koppel - 2019 - Consistent Online Gaussian Process Regression With.pdf:C\:\\Users\\pnter\\Zotero\\storage\\89ANUEEW\\Koppel - 2019 - Consistent Online Gaussian Process Regression With.pdf:application/pdf}
}

@inproceedings{calandra_manifold_2016,
	title = {Manifold {Gaussian} {Processes} for regression},
	doi = {10.1109/IJCNN.2016.7727626},
	abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
	booktitle = {2016 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
	month = jul,
	year = {2016},
	note = {ISSN: 2161-4407},
	keywords = {Bayes methods, Gaussian processes, regression analysis, Training, complex nonsmooth functions, Computational modeling, data handling, data representation, GP regression, manifold Gaussian processes, Manifolds, Standards, Supervised learning, supervised method},
	pages = {3338--3345},
	file = {Accepted Version:C\:\\Users\\pnter\\Zotero\\storage\\FS8TEDEN\\Calandra et al. - 2016 - Manifold Gaussian Processes for regression.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pnter\\Zotero\\storage\\UXD4WFFR\\7727626.html:text/html}
}

@article{colosimo_multisensor_2015,
	title = {Multisensor data fusion via {Gaussian} process models for dimensional and geometric verification},
	volume = {40},
	issn = {0141-6359},
	url = {http://www.sciencedirect.com/science/article/pii/S0141635914002104},
	doi = {10.1016/j.precisioneng.2014.11.011},
	abstract = {An increasing amount of commercial measurement instruments implementing a wide range of measurement technologies is rapidly becoming available for dimensional and geometric verification. Multiple solutions are often acquired within the shop-floor with the aim of providing alternatives to cover a wider array of measurement needs, thus overcoming the limitations of individual instruments and technologies. In such scenarios, multisensor data fusion aims at going one step further by seeking original and different ways to analyze and combine multiple measurement datasets taken from the same measurand, in order to produce synergistic effects and ultimately obtain overall better measurement results. In this work an original approach to multisensor data fusion is presented, based on the development of Gaussian process models (the technique also known as kriging), starting from point sets acquired from multiple instruments. The approach is illustrated and validated through the application to a simulated test case and two real-life industrial metrology scenarios involving structured light scanners and coordinate measurement machines. The results show that not only the proposed approach allows for obtaining final measurement results whose metrological quality transcends that of the original single-sensor datasets, but also it allows to better characterize metrological performance and potential sources of measurement error originated from within each individual sensor.},
	language = {en},
	urldate = {2019-12-18},
	journal = {Precision Engineering},
	author = {Colosimo, Bianca Maria and Pacella, Massimo and Senin, Nicola},
	month = apr,
	year = {2015},
	keywords = {Gaussian process, CMM, Coordinate metrology, Kriging, Multisensor data fusion, Structured light scanner},
	pages = {199--213},
	file = {ScienceDirect Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\GZ75IAE8\\S0141635914002104.html:text/html}
}

@article{filip_smooth_2019,
	title = {Smooth {Random} {Functions}, {Random} {ODEs}, and {Gaussian} {Processes}},
	volume = {61},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/17M1161853},
	doi = {10.1137/17M1161853},
	abstract = {The usual way in which mathematicians work with randomness is by a rigorous formulation of the idea of Brownian motion, which is the limit of a random walk as the step length goes to zero. A Brownian path is continuous but nowhere differentiable, and this nonsmoothness is associated with technical complications that can be daunting. However, there is another approach to random processes that is more elementary, involving smooth random functions defined by finite Fourier series with random coefficients or, equivalently, by trigonometric polynomial interpolation through random data values.  We show here how smooth random functions can provide a very practical way to explore random effects.  For example, one can solve smooth random ordinary differential equations using standard mathematical definitions and numerical algorithms, rather than having to develop new definitions and algorithms of stochastic differential equations. In the limit as the number of Fourier coefficients defining a smooth random function goes to \${\textbackslash}infty\$, one obtains the usual stochastic objects in what is known as their Stratonovich interpretation.},
	number = {1},
	urldate = {2019-12-18},
	journal = {SIAM Review},
	author = {Filip, Silviu. and Javeed, Aurya. and Trefethen, Lloyd N.},
	month = jan,
	year = {2019},
	pages = {185--205},
	file = {Full Text PDF:C\:\\Users\\pnter\\Zotero\\storage\\76VYPD2F\\Filip et al. - 2019 - Smooth Random Functions, Random ODEs, and Gaussian.pdf:application/pdf;Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\K9C9LB28\\17M1161853.html:text/html}
}

@article{srinivas_information-theoretic_2012,
	title = {Information-{Theoretic} {Regret} {Bounds} for {Gaussian} {Process} {Optimization} in the {Bandit} {Setting}},
	volume = {58},
	issn = {1557-9654},
	doi = {10.1109/TIT.2011.2182033},
	abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low norm in a reproducing kernel Hilbert space. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze an intuitive Gaussian process upper confidence bound (GP-UCB) algorithm, and bound its cumulative regret in terms of maximal in- formation gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
	number = {5},
	journal = {IEEE Transactions on Information Theory},
	author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias W.},
	month = may,
	year = {2012},
	keywords = {Bayesian methods, Gaussian process (GP), Gaussian processes, Noise, nonparametric statistics, Convergence, Bandit problems, bandit setting, Bayesian prediction, cumulative regret, experimental design, Gaussian process optimization, Hilbert spaces, information gain, information theory, information-theoretic regret bounds, intuitive Gaussian process upper confidence bound algorithm, Kernel, multiarmed bandit problem, online learning, Optimization, payoff function, regret bound, reproducing kernel Hilbert space, statistical learning, sublinear regret bounds, Temperature sensors},
	pages = {3250--3265},
	file = {Full Text:C\:\\Users\\pnter\\Zotero\\storage\\444WXG7N\\Srinivas et al. - 2012 - Information-Theoretic Regret Bounds for Gaussian P.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pnter\\Zotero\\storage\\BSCWLKDS\\6138914.html:text/html}
}

@article{lederer_posterior_2019,
	title = {Posterior {Variance} {Analysis} of {Gaussian} {Processes} with {Application} to {Average} {Learning} {Curves}},
	url = {http://arxiv.org/abs/1906.01404},
	abstract = {The posterior variance of Gaussian processes is a valuable measure of the learning error which is exploited in various applications such as safe reinforcement learning and control design. However, suitable analysis of the posterior variance which captures its behavior for ﬁnite and inﬁnite number of training data is missing. This paper derives a novel bound for the posterior variance function which requires only local information because it depends only on the number of training samples in the proximity of a considered test point. Furthermore, we prove sufﬁcient conditions which ensure the convergence of the posterior variance to zero. Finally, we demonstrate that the extension of our bound to an average learning bound outperforms existing approaches.},
	language = {en},
	urldate = {2019-12-18},
	journal = {arXiv:1906.01404 [cs, stat]},
	author = {Lederer, Armin and Umlauft, Jonas and Hirche, Sandra},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01404},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lederer et al. - 2019 - Posterior Variance Analysis of Gaussian Processes .pdf:C\:\\Users\\pnter\\Zotero\\storage\\7TSYXG8M\\Lederer et al. - 2019 - Posterior Variance Analysis of Gaussian Processes .pdf:application/pdf}
}

@article{williams_upper_nodate,
	title = {Upper and {Lower} {Bounds} on the {Learning} {Curve} for {Gaussian} {Processes}},
	abstract = {In this paper we introduce and illustrate non-trivial upper and lower bounds on the learning curves for one-dimensional Guassian Processes. The analysis is carried out emphasising the effects induced on the bounds by the smoothness of the random process described by the Modiﬁed Bessel and the Squared Exponential covariance functions. We present an explanation of the early, linearly-decreasing behavior of the learning curves and the bounds as well as a study of the asymptotic behavior of the curves. The effects of the noise level and the lengthscale on the tightness of the bounds are also discussed.},
	language = {en},
	author = {Williams, Christopher K I},
	pages = {26},
	file = {Williams - Upper and Lower Bounds on the Learning Curve for G.pdf:C\:\\Users\\pnter\\Zotero\\storage\\P4D3YLBY\\Williams - Upper and Lower Bounds on the Learning Curve for G.pdf:application/pdf}
}

@article{williams_upper_2000,
	title = {Upper and {Lower} {Bounds} on the {Learning} {Curve} for {Gaussian} {Processes}},
	volume = {40},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007601601278},
	doi = {10.1023/A:1007601601278},
	abstract = {In this paper we introduce and illustrate non-trivial upper and lower bounds on the learning curves for one-dimensional Guassian Processes. The analysis is carried out emphasising the effects induced on the bounds by the smoothness of the random process described by the Modified Bessel and the Squared Exponential covariance functions. We present an explanation of the early, linearly-decreasing behavior of the learning curves and the bounds as well as a study of the asymptotic behavior of the curves. The effects of the noise level and the lengthscale on the tightness of the bounds are also discussed.},
	language = {en},
	number = {1},
	urldate = {2019-12-18},
	journal = {Machine Learning},
	author = {Williams, Christopher K.I. and Vivarelli, Francesco},
	month = jul,
	year = {2000},
	pages = {77--102},
	file = {Springer Full Text PDF:C\:\\Users\\pnter\\Zotero\\storage\\AG264M4K\\Williams and Vivarelli - 2000 - Upper and Lower Bounds on the Learning Curve for G.pdf:application/pdf}
}

@article{opper_general_nodate,
	title = {General {Bounds} on {Bayes} {Errors} for {Regression} with {Gaussian} {Processes}},
	abstract = {Based on a simple convexity lemma, we develop bounds for different types of Bayesian prediction errors for regression with Gaussian processes. The basic bounds are formulated for a fixed training set. Simpler expressions are obtained for sampling from an input distribution which equals the weight function of the covariance kernel, yielding asymptotically tight results. The results are compared with numerical experiments.},
	language = {en},
	author = {Opper, Manfred and Vivarelli, Francesco},
	pages = {7},
	file = {Opper and Vivarelli - General Bounds on Bayes Errors for Regression with.pdf:C\:\\Users\\pnter\\Zotero\\storage\\QUB8NCJN\\Opper and Vivarelli - General Bounds on Bayes Errors for Regression with.pdf:application/pdf}
}

@article{ghassemi_analytic_nodate,
	title = {Analytic {Long}-{Term} {Forecasting} with {Periodic} {Gaussian} {Processes}},
	abstract = {Gaussian processes are a state-of-the-art method for learning models from data. Data with an underlying periodic structure appears in many areas, e.g., in climatology or robotics. It is often important to predict the long-term evolution of such a time series, and to take the inherent periodicity explicitly into account. In a Gaussian process, periodicity can be accounted for by an appropriate kernel choice. However, the standard periodic kernel does not allow for analytic long-term forecasting. To address this shortcoming, we re-parametrize the periodic kernel, which, in combination with a double approximation, allows for analytic longterm forecasting of a periodic state evolution with Gaussian processes. Our model allows for probabilistic long-term forecasting of periodic processes, which can be valuable in Bayesian decision making, optimal control, reinforcement learning, and robotics.},
	language = {en},
	author = {Ghassemi, Nooshin Haji and Deisenroth, Marc Peter},
	pages = {9},
	file = {Ghassemi and Deisenroth - Analytic Long-Term Forecasting with Periodic Gauss.pdf:C\:\\Users\\pnter\\Zotero\\storage\\57TLB8GP\\Ghassemi and Deisenroth - Analytic Long-Term Forecasting with Periodic Gauss.pdf:application/pdf}
}

@article{tulabandhula_machine_nodate,
	title = {Machine {Learning} with {Operational} {Costs}},
	abstract = {This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm’s objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization.},
	language = {en},
	author = {Tulabandhula, Theja and Rudin, Cynthia},
	pages = {40},
	file = {Tulabandhula and Rudin - Machine Learning with Operational Costs.pdf:C\:\\Users\\pnter\\Zotero\\storage\\EVCVQLM4\\Tulabandhula and Rudin - Machine Learning with Operational Costs.pdf:application/pdf}
}

@article{bengio_extensions_nodate,
	title = {Extensions to {Metric}-{Based} {Model} {Selection}},
	abstract = {Metric-based methods have recently been introduced for model selection and regularization, often yielding very signiﬁcant improvements over the alternatives tried (including cross-validation). All these methods require unlabeled data over which to compare functions and detect gross differences in behavior away from the training points. We introduce three new extensions of the metric model selection methods and apply them to feature selection. The ﬁrst extension takes advantage of the particular case of time-series data in which the task involves prediction with a horizon h. The idea is to use at t the h unlabeled examples that precede t for model selection. The second extension takes advantage of the different error distributions of cross-validation and the metric methods: crossvalidation tends to have a larger variance and is unbiased. A hybrid combining the two model selection methods is rarely beaten by any of the two methods. The third extension deals with the case when unlabeled data is not available at all, using an estimated input density. Experiments are described to study these extensions in the context of capacity control and feature subset selection.},
	language = {en},
	author = {Bengio, Yoshua and Chapados, Nicolas},
	pages = {19},
	file = {Bengio and Chapados - Extensions to Metric-Based Model Selection.pdf:C\:\\Users\\pnter\\Zotero\\storage\\AE6V5VSL\\Bengio and Chapados - Extensions to Metric-Based Model Selection.pdf:application/pdf}
}

@article{mannor_online_nodate,
	title = {Online {Learning} with {Sample} {Path} {Constraints}},
	abstract = {We study online learning where a decision maker interacts with Nature with the objective of maximizing her long-term average reward subject to some sample path average constraints. We deﬁne the reward-in-hindsight as the highest reward the decision maker could have achieved, while satisfying the constraints, had she known Nature’s choices in advance. We show that in general the reward-in-hindsight is not attainable. The convex hull of the reward-in-hindsight function is, however, attainable. For the important case of a single constraint, the convex hull turns out to be the highest attainable function. Using a calibrated forecasting rule, we provide an explicit strategy that attains this convex hull. We also measure the performance of heuristic methods based on non-calibrated forecasters in experiments involving a CPU power management problem.},
	language = {en},
	author = {Mannor, Shie and Tsitsiklis, John N and Yu, Jia Yuan},
	pages = {22},
	file = {Mannor et al. - Online Learning with Sample Path Constraints.pdf:C\:\\Users\\pnter\\Zotero\\storage\\6XQQQV2R\\Mannor et al. - Online Learning with Sample Path Constraints.pdf:application/pdf}
}

@article{anava_online_nodate,
	title = {Online {Learning} for {Time} {Series} {Prediction}},
	abstract = {We address the problem of predicting a time series using the ARMA (autoregressive moving average) model, under minimal assumptions on the noise terms. Using regret minimization techniques, we develop eﬀective online learning algorithms for the prediction problem, without assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we show that our algorithm’s performances asymptotically approaches the performance of the best ARMA model in hindsight.},
	language = {en},
	author = {Anava, Oren and Hazan, Elad and Mannor, Shie and Shamir, Ohad},
	pages = {13},
	file = {Anava et al. - Online Learning for Time Series Prediction.pdf:C\:\\Users\\pnter\\Zotero\\storage\\5XMUTUYB\\Anava et al. - Online Learning for Time Series Prediction.pdf:application/pdf}
}

@article{murray_machine_nodate,
	title = {Machine {Learning} {Methods} for {Predicting} {Failures} in {Hard} {Drives}: {A} {Multiple}-{Instance} {Application}},
	abstract = {We compare machine learning methods applied to a difﬁcult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classiﬁer (mi-NB) which is speciﬁcally designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum signiﬁcance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures.},
	language = {en},
	author = {Murray, Joseph F and Hughes, Gordon F and Kreutz-Delgado, Kenneth},
	pages = {34},
	file = {Murray et al. - Machine Learning Methods for Predicting Failures i.pdf:C\:\\Users\\pnter\\Zotero\\storage\\SVP7S4G4\\Murray et al. - Machine Learning Methods for Predicting Failures i.pdf:application/pdf}
}

@incollection{dzeroski_variational_2014,
	address = {Cham},
	title = {Variational {Dependent} {Multi}-output {Gaussian} {Process} {Dynamical} {Systems}},
	volume = {8777},
	isbn = {978-3-319-11811-6 978-3-319-11812-3},
	url = {http://link.springer.com/10.1007/978-3-319-11812-3_30},
	abstract = {This paper presents a dependent multi-output Gaussian process (GP) for modeling complex dynamical systems. The outputs are dependent in this model, which is largely diﬀerent from previous GP dynamical systems. We adopt convolved multi-output GPs to model the outputs, which are provided with a ﬂexible multi-output covariance function. We adapt the variational inference method with inducing points for learning the model. Conjugate gradient based optimization is used to solve parameters involved by maximizing the variational lower bound of the marginal likelihood. The proposed model has superiority on modeling dynamical systems under the more reasonable assumption and the fully Bayesian learning framework. Further, it can be ﬂexibly extended to handle regression problems. We evaluate the model on both synthetic and real-world data including motion capture data, traﬃc ﬂow data and robot inverse dynamics data. Various evaluation methods are taken on the experiments to demonstrate the eﬀectiveness of our model, and encouraging results are observed.},
	language = {en},
	urldate = {2019-12-30},
	booktitle = {Discovery {Science}},
	publisher = {Springer International Publishing},
	author = {Zhao, Jing and Sun, Shiliang},
	editor = {Džeroski, Sašo and Panov, Panče and Kocev, Dragi and Todorovski, Ljupčo},
	year = {2014},
	doi = {10.1007/978-3-319-11812-3_30},
	pages = {350--361},
	file = {Zhao and Sun - 2014 - Variational Dependent Multi-output Gaussian Proces.pdf:C\:\\Users\\pnter\\Zotero\\storage\\36CFBQLE\\Zhao and Sun - 2014 - Variational Dependent Multi-output Gaussian Proces.pdf:application/pdf}
}

@article{ambikasaran_fast_2015,
	title = {Fast {Direct} {Methods} for {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1403.6015},
	abstract = {A number of problems in probability and statistics can be addressed using the multivariate normal (Gaussian) distribution. In the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding Gaussian density. In the n-dimensional setting, however, it requires the inversion of an n × n covariance matrix, C, as well as the evaluation of its determinant, det(C). In many cases, such as regression using Gaussian processes, the covariance matrix is of the form C = σ2I + K, where K is computed using a speciﬁed covariance kernel which depends on the data and additional parameters (hyperparameters). The matrix C is typically dense, causing standard direct methods for inversion and determinant evaluation to require O(n3) work. This cost is prohibitive for large-scale modeling. Here, we show that for the most commonly used covariance functions, the matrix C can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an O(n log2 n) algorithm for inversion. More importantly, we show that this factorization enables the evaluation of the determinant det(C), permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel deﬁning K. Our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single CPU core. The combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. We illustrate the performance of the scheme on standard covariance kernels.},
	language = {en},
	urldate = {2019-12-31},
	journal = {arXiv:1403.6015 [astro-ph, stat]},
	author = {Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W. and O'Neil, Michael},
	month = apr,
	year = {2015},
	note = {arXiv: 1403.6015},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Mathematics - Numerical Analysis, Mathematics - Statistics Theory},
	file = {Ambikasaran et al. - 2015 - Fast Direct Methods for Gaussian Processes.pdf:C\:\\Users\\pnter\\Zotero\\storage\\CVZ2FLNQ\\Ambikasaran et al. - 2015 - Fast Direct Methods for Gaussian Processes.pdf:application/pdf}
}

@article{wang_backpropagation-friendly_nodate,
	title = {Backpropagation-{Friendly} {Eigendecomposition}},
	abstract = {Eigendecomposition (ED) is widely used in deep networks. However, the backpropagation of its results tends to be numerically unstable, whether using ED directly or approximating it with the Power Iteration method, particularly when dealing with large matrices. While this can be mitigated by partitioning the data in small and arbitrary groups, doing so has no theoretical basis and makes its impossible to exploit the power of ED to the full.},
	language = {en},
	author = {Wang, Wei and Dang, Zheng and Hu, Yinlin and Fua, Pascal and Salzmann, Mathieu},
	pages = {9},
	file = {Wang et al. - Backpropagation-Friendly Eigendecomposition.pdf:C\:\\Users\\pnter\\Zotero\\storage\\78N3ANCU\\Wang et al. - Backpropagation-Friendly Eigendecomposition.pdf:application/pdf}
}

@article{seeger_low_nodate,
	title = {Low {Rank} {Updates} for the {Cholesky} {Decomposition}},
	abstract = {Usage of the Sherman-Morrison-Woodbury formula to update linear systems after low rank modiﬁcations of the system matrix is widespread in machine learning. However, it is well known that this formula can lead to serious instabilities in the presence of roundoﬀ error. If the system matrix is symmetric positive deﬁnite, it is almost always possible to use a representation based on the Cholesky decomposition which renders the same results (in exact arithmetic) at the same or less operational cost, but typically is much more numerically stable. In this note, we show how the Cholesky decomposition can be updated to incorporate low rank additions or downdated for low rank subtractions. We also discuss a special case of an indeﬁnite update of rank two. The methods discussed here are well-known in the numerical mathematics literature, and code for most of them can be found in the LINPACK suite.},
	language = {en},
	author = {Seeger, Matthias},
	pages = {7},
	file = {Seeger - Low Rank Updates for the Cholesky Decomposition.pdf:C\:\\Users\\pnter\\Zotero\\storage\\URGFJG4A\\Seeger - Low Rank Updates for the Cholesky Decomposition.pdf:application/pdf}
}

@article{quinonero-candela_approximation_nodate,
	title = {Approximation {Methods} for {Gaussian} {Process} {Regression}},
	abstract = {A wealth of computationally eﬃcient approximation methods for Gaussian process regression have been recently proposed. We give a unifying overview of sparse approximations, following Quin˜onero-Candela and Rasmussen (2005), and a brief review of approximate matrix-vector multiplication methods.},
	language = {en},
	author = {Quinonero-Candela, Joaquin and Ramussen, Carl Edward and Williams, Christopher K I},
	pages = {24},
	file = {Quinonero-Candela et al. - Approximation Methods for Gaussian Process Regress.pdf:C\:\\Users\\pnter\\Zotero\\storage\\NCLJYIBS\\Quinonero-Candela et al. - Approximation Methods for Gaussian Process Regress.pdf:application/pdf}
}

@article{nguyen-tuong_local_2009,
	title = {Local {Gaussian} {Process} {Regression} for {Real} {Time} {Online} {Model} {Learning}},
	abstract = {Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR.},
	language = {en},
	journal = {Advances  in  Neural  Information  ProcessingSystems},
	author = {Nguyen-Tuong, Duy and Peters, Jan R and Seeger, Matthias},
	year = {2009},
	pages = {1193--1200}
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difﬁcult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to ﬁrst posit a family of densities and then to ﬁnd the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-ﬁeld variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	language = {en},
	number = {518},
	urldate = {2020-02-18},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	pages = {859--877},
	file = {Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:C\:\\Users\\pnter\\Zotero\\storage\\9CSYR2KD\\Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf}
}

@article{titsias_variational_nodate,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are deﬁned to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	author = {Titsias, Michalis K},
	pages = {8},
	file = {Titsias - Variational Learning of Inducing Variables in Spar.pdf:C\:\\Users\\pnter\\Zotero\\storage\\NT3YAMB5\\Titsias - Variational Learning of Inducing Variables in Spar.pdf:application/pdf}
}

@article{savaresi_comparative_2004,
	title = {A comparative analysis on the bisecting {K}-means and the {PDDP} clustering algorithms},
	volume = {8},
	issn = {15714128, 1088467X},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-2004-8403},
	doi = {10.3233/IDA-2004-8403},
	abstract = {This paper deals with the problem of clustering a data set. In particular, the bisecting divisive partitioning approach is here considered. We focus on two algorithms: the celebrated K-means algorithm, and the recently proposed Principal Direction Divisive Partitioning (PDDP) algorithm. A comparison of the two algorithms is given, under the assumption that the data set is uniformly distributed within an ellipsoid. In particular, the dynamic behavior of the K-means iterative procedure is studied and discussed; for the 2-dimensional case a closed-form model is given.},
	language = {en},
	number = {4},
	urldate = {2020-02-27},
	journal = {Intelligent Data Analysis},
	author = {Savaresi, Sergio M. and Boley, Daniel L.},
	month = oct,
	year = {2004},
	pages = {345--362},
	file = {Savaresi and Boley - 2004 - A comparative analysis on the bisecting K-means an.pdf:C\:\\Users\\pnter\\Zotero\\storage\\33V7NQ9X\\Savaresi and Boley - 2004 - A comparative analysis on the bisecting K-means an.pdf:application/pdf}
}

@article{uykan_analysis_2000,
	title = {Analysis of input-output clustering for determining centers of {RBFN}},
	volume = {11},
	issn = {1941-0093},
	doi = {10.1109/72.857766},
	abstract = {The key point in design of radial basis function networks is to specify the number and the locations of the centers. Several heuristic hybrid learning methods, which apply a clustering algorithm for locating the centers and subsequently a linear least-squares method for the linear weights, have been previously suggested. These hybrid methods can be put into two groups, which will be called as input clustering (IC) and input-output clustering (IOC), depending on whether the output vector is also involved in the clustering process. The idea of concatenating the output vector to the input vector in the clustering process has independently been proposed by several papers in the literature although none of them presented a theoretical analysis on such procedures, but rather demonstrated their effectiveness in several applications. The main contribution of this paper is to present an approach for investigating the relationship between clustering process on input-output training samples and the mean squared output error in the context of a radial basis function network (RBFN). We may summarize our investigations in that matter as follows: (1) A weighted mean squared input-output quantization error, which is to be minimized by IOC, yields an upper bound to the mean squared output error. (2) This upper bound and consequently the output error can be made arbitrarily small (zero in the limit case) by decreasing the quantization error which can be accomplished through increasing the number of hidden units.},
	number = {4},
	journal = {IEEE Transactions on Neural Networks},
	author = {Uykan, Z. and Guzelis, C. and Celebi, M.E. and Koivo, H.N.},
	month = jul,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {pattern clustering, learning (artificial intelligence), Kernel, Clustering algorithms, heuristic hybrid learning methods, heuristic programming, Hybrid integrated circuits, I/O clustering, input-output clustering, Interpolation, IOC, Learning systems, least squares approximations, linear least-squares method, mean squared output error, minimisation, Multilayer perceptrons, Quantization, radial basis function network design, radial basis function networks, Radial basis function networks, RBFN centers, Upper bound, Vectors, weighted mean squared input-output quantization error},
	pages = {851--858},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\pnter\\Zotero\\storage\\A9TCSYCD\\857766.html:text/html}
}

@article{gallier_schur_nodate,
	title = {The {Schur} {Complement} and {Symmetric} {Positive} {Semideﬁnite} (and {Deﬁnite}) {Matrices}},
	language = {en},
	author = {Gallier, Jean},
	pages = {13},
	file = {Gallier - The Schur Complement and Symmetric Positive Semide.pdf:C\:\\Users\\pnter\\Zotero\\storage\\ISGKN796\\Gallier - The Schur Complement and Symmetric Positive Semide.pdf:application/pdf}
}

@article{cheng_model-selection-based_2004,
	title = {A {Model}-{Selection}-{Based} {Self}-{Splitting} {Gaussian} {Mixture} {Learning} with {Application} to {Speaker} {Identification}},
	volume = {2004},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704407100},
	doi = {10.1155/S1110865704407100},
	abstract = {We propose a self-splitting Gaussian mixture learning (SGML) algorithm for Gaussian mixture modelling. The SGML algorithm is deterministic and is able to ﬁnd an appropriate number of components of the Gaussian mixture model (GMM) based on a self-splitting validity measure, Bayesian information criterion (BIC). It starts with a single component in the feature space and splits adaptively during the learning process until the most appropriate number of components is found. The SGML algorithm also performs well in learning the GMM with a given component number. In our experiments on clustering of a synthetic data set and the text-independent speaker identiﬁcation task, we have observed the ability of the SGML for model-based clustering and automatically determining the model complexity of the speaker GMMs for speaker identiﬁcation.},
	language = {en},
	number = {17},
	urldate = {2020-04-07},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Cheng, Shih-Sian and Wang, Hsin-Min and Fu, Hsin-Chia},
	month = dec,
	year = {2004},
	pages = {312192},
	file = {Cheng et al. - 2004 - A Model-Selection-Based Self-Splitting Gaussian Mi.pdf:C\:\\Users\\pnter\\Zotero\\storage\\YQEQ8E5T\\Cheng et al. - 2004 - A Model-Selection-Based Self-Splitting Gaussian Mi.pdf:application/pdf}
}

@article{cawley_preventing_nodate,
	title = {Preventing {Over}-{Fitting} during {Model} {Selection} via {Bayesian} {Regularisation} of the {Hyper}-{Parameters}},
	abstract = {While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-ﬁtting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach.},
	language = {en},
	author = {Cawley, Gavin C and Talbot, Nicola L C},
	pages = {21},
	file = {Cawley and Talbot - Preventing Over-Fitting during Model Selection via.pdf:C\:\\Users\\pnter\\Zotero\\storage\\IZAU5VVU\\Cawley and Talbot - Preventing Over-Fitting during Model Selection via.pdf:application/pdf}
}

@incollection{zhang_learning_nodate,
	title = {Learning {Uncertainty} using {Clustering} and {Local} {Gaussian} {Process} {Regression}},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2019-1730},
	urldate = {2020-04-14},
	booktitle = {{AIAA} {Scitech} 2019 {Forum}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Zhang, Yiming and Ghosh, Sayan and Asher, Isaac and Ling, You and Wang, Liping},
	doi = {10.2514/6.2019-1730},
	note = {\_eprint: https://arc.aiaa.org/doi/pdf/10.2514/6.2019-1730},
	file = {AIAA Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\26HIG2YW\\6.html:text/html}
}

@article{solak_derivative_nodate,
	title = {Derivative {Observations} in {Gaussian} {Process} {Models} of {Dynamic} {Systems}},
	abstract = {Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identiﬁcation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors speciﬁed by an expert or identiﬁed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efﬁciency of Gaussian process models for dynamic system identiﬁcation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size – traditionally a problem for Gaussian process models.},
	language = {en},
	author = {Solak, E and Murray-smith, R and Leithead, W E and Leith, D J and Rasmussen, Carl E},
	pages = {8},
	file = {Solak et al. - Derivative Observations in Gaussian Process Models.pdf:C\:\\Users\\pnter\\Zotero\\storage\\BMIBAW7X\\Solak et al. - Derivative Observations in Gaussian Process Models.pdf:application/pdf}
}

@inproceedings{snelson_local_2007,
	title = {Local and global sparse {Gaussian} process approximations},
	url = {http://proceedings.mlr.press/v2/snelson07a.html},
	abstract = {Gaussian process (GP) models are flexible probabilistic nonparametric models for regression, classification and other tasks. Unfortunately they suffer from computational intractability for large da...},
	language = {en},
	urldate = {2020-04-21},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	month = mar,
	year = {2007},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {524--531},
	file = {Full Text PDF:C\:\\Users\\pnter\\Zotero\\storage\\IKBFXMBU\\Snelson and Ghahramani - 2007 - Local and global sparse Gaussian process approxima.pdf:application/pdf;Snapshot:C\:\\Users\\pnter\\Zotero\\storage\\2ED77X9G\\snelson07a.html:text/html}
}

@article{rasmussen_infinite_2002,
	title = {Infinite {Mixtures} of {Gaussian} {Process} {Experts}},
	abstract = {We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an inﬁnite number of Experts. Inference in this model may be done efﬁciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets – thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach.},
	language = {en},
	number = {14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Rasmussen, Carl E and Ghahramani, Zoubin},
	year = {2002},
	pages = {881--888},
	file = {Rasmussen and Ghahramani - Infinite Mixtures of Gaussian Process Experts.pdf:C\:\\Users\\pnter\\Zotero\\storage\\FVKKR24P\\Rasmussen and Ghahramani - Infinite Mixtures of Gaussian Process Experts.pdf:application/pdf}
}

@article{ng_hierarchical_2014,
	title = {Hierarchical {Mixture}-of-{Experts} {Model} for {Large}-{Scale} {Gaussian} {Process} {Regression}},
	url = {http://arxiv.org/abs/1412.3078},
	abstract = {We propose a practical and scalable Gaussian process model for large-scale nonlinear probabilistic regression. Our mixture-of-experts model is conceptually simple and hierarchically recombines computations for an overall approximation of a full Gaussian process. Closed-form and distributed computations allow for efﬁcient and massive parallelisation while keeping the memory consumption small. Given sufﬁcient computing resources, our model can handle arbitrarily large data sets, without explicit sparse approximations. We provide strong experimental evidence that our model can be applied to large data sets of sizes far beyond millions. Hence, our model has the potential to lay the foundation for general large-scale Gaussian process research.},
	language = {en},
	urldate = {2020-04-21},
	journal = {arXiv:1412.3078 [cs, stat]},
	author = {Ng, Jun Wei and Deisenroth, Marc Peter},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3078},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Computer Science - Artificial Intelligence},
	file = {Ng and Deisenroth - 2014 - Hierarchical Mixture-of-Experts Model for Large-Sc.pdf:C\:\\Users\\pnter\\Zotero\\storage\\2GWMZ4GC\\Ng and Deisenroth - 2014 - Hierarchical Mixture-of-Experts Model for Large-Sc.pdf:application/pdf}
}

@article{deisenroth_distributed_2015,
	title = {Distributed {Gaussian} {Processes}},
	abstract = {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-theart sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efﬁcient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufﬁcient computing resources our distributed GP model can handle arbitrarily large data sets.},
	language = {en},
	journal = {Proceedings ofthe 32 nd International Conference on Machine Learning},
	author = {Deisenroth, Marc Peter and Ng, Jun Wei},
	year = {2015},
	pages = {1--10},
	file = {Deisenroth and Ng - Distributed Gaussian Processes.pdf:C\:\\Users\\pnter\\Zotero\\storage\\SN5X2U37\\Deisenroth and Ng - Distributed Gaussian Processes.pdf:application/pdf}
}

@inproceedings{gardner_gpytorch_2018,
	address = {Montréal, Canada},
	series = {{NIPS}'18},
	title = {{GPyTorch}: blackbox matrix-matrix {Gaussian} process inference with {GPU} acceleration},
	shorttitle = {{GPyTorch}},
	abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
	urldate = {2020-04-26},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = dec,
	year = {2018},
	pages = {7587--7597},
	file = {Full Text PDF:C\:\\Users\\pnter\\Zotero\\storage\\I5RA55ZM\\Gardner et al. - 2018 - GPyTorch blackbox matrix-matrix Gaussian process .pdf:application/pdf}
}

@article{boley_principal_1998,
	title = {Principal {Direction} {Divisive} {Partitioning}},
	volume = {2},
	issn = {1573-756X},
	url = {https://doi.org/10.1023/A:1009740529316},
	doi = {10.1023/A:1009740529316},
	abstract = {We propose a new algorithm capable of partitioning a set of documents or other samples based on an embedding in a high dimensional Euclidean space (i.e., in which every document is a vector of real numbers). The method is unusual in that it is divisive, as opposed to agglomerative, and operates by repeatedly splitting clusters into smaller clusters. The documents are assembled into a matrix which is very sparse. It is this sparsity that permits the algorithm to be very efficient. The performance of the method is illustrated with a set of text documents obtained from the World Wide Web. Some possible extensions are proposed for further investigation.},
	number = {4},
	journal = {Data Mining and Knowledge Discovery},
	author = {Boley, Daniel},
	month = dec,
	year = {1998},
	pages = {325--344}
}

@article{park_patchwork_2018,
	title = {Patchwork {Kriging} for {Large}-scale {Gaussian} {Process} {Regression}},
	volume = {19},
	url = {http://jmlr.org/papers/v19/17-042.html},
	number = {7},
	journal = {Journal of Machine Learning Research},
	author = {Park, Chiwoo and Apley, Daniel},
	year = {2018},
	pages = {1--43}
}

@article{park_efficient_2016,
	title = {Efficient {Computation} of {Gaussian} {Process} {Regression} for {Large} {Spatial} {Data} {Sets} by {Patching} {Local} {Gaussian} {Processes}},
	volume = {17},
	url = {http://jmlr.org/papers/v17/15-327.html},
	number = {174},
	journal = {Journal of Machine Learning Research},
	author = {Park, Chiwoo and Huang, Jianhua Z.},
	year = {2016},
	pages = {1--29}
}

@article{park_domain_2011,
	title = {Domain {Decomposition} {Approach} for {Fast} {Gaussian} {Process} {Regression} of {Large} {Spatial} {Data} {Sets}},
	volume = {12},
	url = {http://jmlr.org/papers/v12/park11a.html},
	number = {47},
	journal = {Journal of Machine Learning Research},
	author = {Park, Chiwoo and Huang, Jianhua Z. and Ding, Yu},
	year = {2011},
	pages = {1697--1728}
}